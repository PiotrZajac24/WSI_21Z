# WSI - LAB 4 - Piotr Zaj�cimport pandas as pdfrom math import logfrom copy import deepcopyimport numpy as npimport seaborn as snsfrom sklearn.metrics import confusion_matriximport matplotlib.pyplot as pltimport jsonfrom collections import defaultdictimport argparsefrom pathlib import Pathclass Node:    def __init__(self, parent=None, value=None):        self.attribute = None        self.value = value        self.parent = parent        self.descendants = {}        self.filters = {}def recall(tp, fn):    return round(tp/(tp+fn), 2) if tp+fn > 0 else np.nandef precision(tp, fp):    return round(tp/(tp+fp), 2) if tp+fp > 0 else np.nandef accuracy(tp, all_r):    return round(tp/all_r, 2) if all_r > 0 else np.nanclass ID3Tree:    def __init__(self, train_data):        self.root = Node()        self.nodes = []        self._attributes = set()        self._data = train_data        self._load_attributes()    def _load_attributes(self):        self._attributes = set(self._data.columns[:-1])        self._class = self._data.columns[-1]    def _build_tree(self, node, data):        if len(data) == 0:  # check if data for current branch exists            return        count = data[data.columns[-1]].value_counts()        if len(count) == 1: # check if data for current branch contains only one class            node.value = count.index.tolist()[0]            return        attributes_left = self._attributes.difference(set(node.filters.keys()))        if not attributes_left: # check if there are any attributes left            if len(count) > 0:                node.value = count.index.tolist()[0]            return        mx = -1        best_attr = None        for attr in attributes_left:    # calculate information gain for each attribute and select the best one            attr_gain = self.info_gain(attr, data)            if attr_gain > mx:                mx = attr_gain                best_attr = attr        node.attribute = best_attr  # set current node attribute        attr_values = data[node.attribute].unique()        for val in attr_values:     # for each attribute value create subtree from filtered data            node.descendants[val] = Node(node)            node.descendants[val].filters = deepcopy(node.filters)            node.descendants[val].filters[best_attr] = val            new_data = data[data[best_attr] == val]            self._build_tree(node.descendants[val], new_data)        # find most common class, in case some attribute values were not used in current branch        most_common = data[data.columns[-1]].value_counts().index.tolist()[0]        node.descendants["default"] = Node(node, most_common)    def build_tree(self):        self._build_tree(self.root, self._data)    def entropy(self, data):        l= len(data)        result = 0        class_count = data[data.columns[-1]].value_counts()        for _, val in class_count.items():            p = val/l            if p > 0:                result -= p*log(p)        return result    def info(self, attribute, data):        attr_count = data[attribute].value_counts()        l = len(data)        result = 0        for attr, val in attr_count.items():            p = val/l            if p > 0:                result += p * self.entropy(data[data[attribute] == attr])        return result    def info_gain(self, attribute, data):        return self.entropy(data) - self.info(attribute, data)    def test_tree(self, test_data, generate_heatmap=False):        expected = test_data[test_data.columns[-1]]        predicted = []        # for each row in testing data, get predicted class         for i, sample in test_data.iterrows():            node = self.root            while True:                node = node.descendants.get(sample[node.attribute], node.descendants["default"])                if node.value:                    predicted.append(node.value)                    break        labels = sorted(list(set(test_data[test_data.columns[-1]])))        cf_matrix = confusion_matrix(expected, predicted, labels=labels)        # calculate metrics for given predictions        all_results = {}        all_tp = 0        s = sum(sum(cf_matrix))        for i, class_ in enumerate(labels):            tp = cf_matrix[i, i]            fn = sum(cf_matrix[i]) - tp            fp = sum(cf_matrix[:, i]) - tp            tn = s - fn - fp - tp            all_tp += tp            all_results[class_] = {                "recall": recall(tp, fn),                "precision": precision(tp, fp),                "all_cases": {"tp": int(tp), "tn": int(tn), "fp":int(fp), "fn":int(fn)}            }        if generate_heatmap:            ax = sns.heatmap(cf_matrix, annot=True, color="Green", fmt='g', xticklabels=labels, yticklabels=labels)            ax.xaxis.set_ticks_position("top")            plt.title("Confusion matrix")            plt.xlabel("Predicted", fontsize=12)            plt.ylabel("Actual", fontsize=12)            plt.show()        return all_results, accuracy(all_tp, s), cf_matrix.tolist()class CrossValidation:    def __init__(self, data, k):        self._data = data        self.k = k        self._sets = []    def generate_sets(self):        self._sets.clear()        length = len(self._data)        set_length = length//self.k        remaining = len(self._data) % self.k        for i in range(self.k):            lower = i*(set_length+1) if i < remaining else remaining*(set_length+1) + (i - remaining)*set_length            upper = lower + set_length + (1 if i < remaining else 0)            self._sets.append(self._data.iloc[lower:upper])        return self._setsdef parse_arguments():    parser = argparse.ArgumentParser()    parser.add_argument('--file', default="./data/car_data.csv", type=str)    args = parser.parse_args()    if Path(args.file).suffix != ".csv":        raise ValueError("Csv file extension required.")    return argsdef main():    args = parse_arguments()    data = pd.read_csv(args.file)    all_results = {"no_cross": {}, "cross": {}}    # no cross validation    for i in np.arange(0.1, 1, 0.1):        i = round(i, 2) # percent of training data in the whole set        print("training data: ", i)        temp_results = {            "metrics": defaultdict(lambda: {"recall": [], "precision": []}),            "accuracy": []        }        for _ in range(15):            train = data.sample(frac=i)            test = data.drop(train.index)            tree = ID3Tree(train)            tree.build_tree()            class_metrics, accuracy_, cf_matrix = tree.test_tree(test, False)            for k, v in class_metrics.items():                temp_results["metrics"][k]["recall"].append(class_metrics[k]["recall"])                temp_results["metrics"][k]["precision"].append(class_metrics[k]["precision"])            temp_results["accuracy"].append(accuracy_)        aggr_res = {            class_: {                met:{                    "mean": round(np.mean(arr), 2),                    "std": round(np. std(arr), 2),                    "min": round(np.min(arr), 2),                    "max": round(np.max(arr), 2)                }                for met, arr in res.items()            }            for class_, res in temp_results["metrics"].items()        }        all_results["no_cross"][i] = {            "metrics": aggr_res,            "accuracy": {                    "mean": round(np.mean(temp_results["accuracy"]), 2),                    "std": round(np.std(temp_results["accuracy"]), 2),                    "min": round(np.min(temp_results["accuracy"]), 2),                    "max": round(np.max(temp_results["accuracy"]), 2)                }        }    # k-cross validation    for k in range(2, 21):        data = pd.read_csv(args.file)        data = data.sample(frac=1)        cv = CrossValidation(data, k)        sets = cv.generate_sets()        accuracies = []        matrices = []        results = defaultdict(lambda: [])        print("k =", k)        for i in range(k):            test_data = sets[i]            training_data = pd.concat(s for j, s in enumerate(sets) if j != i)            tree = ID3Tree(training_data)            tree.build_tree()            class_metrics, accuracy_, cf_matrix = tree.test_tree(test_data, False)            accuracies.append(accuracy_)            matrices.append(cf_matrix)            for class_, res in class_metrics.items():                results[class_].append(res)        all_results["cross"][k] = {"classes": results, "accuracies": accuracies}    with open("./results/results_cars.json", 'w+') as f:        json.dump(all_results, f)if __name__ == "__main__":    main()